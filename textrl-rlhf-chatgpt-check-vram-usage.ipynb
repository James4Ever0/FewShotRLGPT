{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#check if we can change shit.\n# address issue here:\n# https://www.kaggle.com/product-feedback/325765#2073421\n# after done, click \"restart session\" from run tab.\n!pip uninstall pyarrow -y\n!pip install pyarrow==10.0.1\n# !conda install -c conda-forge pyarrow -y\n# remove default pyarrow!\nmpath = \"/opt/conda/lib/python3.7/site-packages/\"\n# under \"pyarrow\"\nimport os\ndirs = os.listdir(mpath)\nfor d in dirs:\n    if \"pyarrow\" in d:\n        print(d)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyarrow\n# pyarrow.__version__ = '10.0.1'\nprint(\"version?\",pyarrow.__version__)\nprint(\"file?\", pyarrow.__file__)\n# let's hack.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# !pip install pyarrow==10.0.1\n# https://www.kaggle.com/product-feedback/325765\n# \n# !conda update pyarrow -y\nimport pyarrow\nprint(\"pyarrow version:\", pyarrow.__version__)\n!pip show pyarrow\nimport pyarrow\nimport os\n\nprint(f\"\\npath: {os.path.abspath(pyarrow.__file__)}\\nversion: {pyarrow.__version__}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pfrl@git+https://github.com/voidful/pfrl.git\n!pip install textrl==0.1.6\n!pip install datasets\n# you don't fucking upgrade!\n!pip install gpuinfo\n!pip install -U sentence-transformers\n!pip install setfit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom setfit import SetFitModel\n# train new model later.\nmodel_id = \"lewtun/my-awesome-setfit-model\"\nmodel = SetFitModel.from_pretrained(model_id)\n# model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getVRAMUsage():\n    from gpuinfo import GPUInfo\n    memusage = GPUInfo.gpu_usage()\n    mdict = dict(zip(*memusage))\n    # this key is unstable. it cannot always get the first GPU card.\n    print(\"dict:\",mdict)\n    data = mdict[list(mdict.keys())[0]]\n    print(\"VRAM?\",data,type(data)) # 1549. around.\n    return data\n\ngetVRAMUsage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getVRAMUsage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test how to use this setfit model\nsetfit_model = model\n# this is just sentence transformer. does it works with long query?\n# i wonder what \"prompt-free\" looks like.\nsetfit_test_data = [\"i loved the spiderman movie!\", \"pineapple on pizza is the worst ðŸ¤®\"]\nval =  setfit_model(setfit_test_data)\nprint(val)\n# what about certainties?\n# this is binary. not so good for reward or adjustments.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dir(model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = model.predict_proba(setfit_test_data)\n# now you have it.\n# does it fucking works?\nprint(val,type(val),val.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# what is the actual probability, when compared as a whole?\nimport numpy\ndef getPositiveProbability(msent_in_array): # multiple elements maybe?\n    val = setfit_model.predict_proba(msent_in_array)\n    # let's not do this shall we?\n    # whatever.\n#     quad = val**2\n    quad = val\n    msum = numpy.sum(quad,axis=1)\n    mval = quad[:,0] # what to do? elon musk speaks evil\n    result = mval/msum\n    return result\n    # but this is something else.\n\nval = getPositiveProbability(setfit_test_data)\nprint(val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textrl import TextRLEnv,TextRLActor\nfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer, AutoModelWithLMHead\nimport logging\nimport sys\nimport pfrl\nimport torch\nlogging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n# reference: https://github.com/voidful/fastpages/blob/master/_notebooks/2022-12-10-textrl-elon-musk.ipynb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on kaggle this is not\n\n# you should mod this.\n# local_files_only=True -> local_files_first\n# try to modify it somehow?\n# you would change the model\n# modelId = \"huggingtweets/elonmusk\"\nmodelId = \"EleutherAI/gpt-neo-125M\"\ntokenizer = AutoTokenizer.from_pretrained(modelId)\nmodel = AutoModelWithLMHead.from_pretrained(modelId)\nmodel.eval()\nmodel.cuda()\nprint(\"model loaded. please check VRAM.\")\ngetVRAMUsage()\n# why don't you print it?","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## warning! vram size is too big. close to 1.5GB even if not training.\n## you'd better do this experiment on kaggle. \n## yes we better shift. change the code if needed. monitor ram usage.\n\n# but what's the capacity? 16GB max for P100 kaggle.\n\n# sentiment = pipeline('sentiment-analysis',model=\"cardiffnlp/twitter-roberta-base-sentiment\",tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment\",device=0,return_all_scores=True)\n# # shit! what to do? revert?\n# # OOM.\n# # let's revert. fuck.\n# getVRAMUsage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get python env version?\n# see if we can use walrus here.\nimport sys\nprint(sys.version) # shit. 3.7 < 3.8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyRLEnv(TextRLEnv):\n    def get_reward(self, input_item, predicted_list, finish): # predicted will be the list of predicted token\n      reward = 0\n      if finish or len(predicted_list) >= self.env_max_length:\n        if 1 < len(predicted_list):\n          print('finished?',finish)\n          print('response length?', len(predicted_list)) # counted for token.\n          predicted_text = tokenizer.convert_tokens_to_string(predicted_list)\n          # sentiment classifier\n          # reward += sentiment(input_item[0]+predicted_text)[0][0]['score']\n          # compute random reward. let's compute according to the letter 's'\n          # count how many letter 's' inside the response.\n          minput = input_item[0]\n          print(\"input?\",minput)\n          mpredicted = predicted_text\n          print('predicted?',mpredicted)\n          total = minput+mpredicted\n#           score = total.lower().count(\"s\") / len(total)\n          # now let's fix.\n          score = getPositiveProbability([total])[0]\n          score = float(score)\n          print(\"score?\",score)\n          reward += score\n      return reward\n\nprint(\"env prepared?\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# different from original observation!\n# i like typo.\nobservaton_list = [['i think dogecoin is']]\n# observaton_list = [['i think dogecoin is', 'but it is not just about the dogecoin,']] # what is this format? dialog?\n# obviously not markov.\n# you cannot know what the model says next. you will only know if it responds and then you may reply.\n# this behavior is online-learning, not in the damn dataset.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nenv = MyRLEnv(model, tokenizer, observation_input=observaton_list)\nactor = TextRLActor(env,model,tokenizer)\nagent = actor.agent_ppo(update_interval=10, minibatch_size=10, epochs=10)\nprint('agent loaded')\ngetVRAMUsage() # 1493. economic?","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myInput = observaton_list[0]\nprint(\"input?\", myInput)\nans = actor.predict(myInput)\n# is it a training batch? what about the model temperature?\nprint(\"ans?\", ans)\ngetVRAMUsage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"about to train. please check VRAM.\")\n# how is the input consumed, by passing two string elements as a single list to `actor.predict`?\n# shit now let's train!\npfrl.experiments.train_agent_with_evaluation(\n    agent,\n    env,\n    steps=300,\n    eval_n_steps=None,\n    eval_n_episodes=1,       \n    train_max_episode_len=100, \n    save_best_so_far_agent=True,\n    checkpoint_freq = 10*10000000000, # almost never save checkpoints.\n    eval_interval=10,\n    outdir='elon_musk_dogecoin', \n) # how to preserve best only?\ngetVRAMUsage()\n# after training?","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# little brain. no valuable response.\n\n# not too much! now let's check disk usage.\n# ok now it is good. how comes?\nagent.load(\"./elon_musk_dogecoin/best\")\n# check vram.\ngetVRAMUsage()\nval = actor.predict(observaton_list[0])\nprint(\"VAL?\",val)\n# so it does include more letter 's' while maintains readability?","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we try to replace the thing with more advanced shits. we score this thing.\n# score labeled as such:\n# same category, with qualified response.\n# sure?\n\n# also you might want to collect data then train.\n# making it into ever-evolving shit.\n# unsupervised -> supervised -> RL\n# a whole loop.\n\n# how to make it granualized?\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}