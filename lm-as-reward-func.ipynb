{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# there are two types of hacks on using autoregressive models as RM.\n# one is to give a prompt and compare which one is better.\n# (hey you can do this with robert, it has \"multiple choice\" capabilities. builtin!)\n# (usually this is not good, since we will have lengthy answers with newline, breaking syntax)\n# (unless you use special token as seen in ELI5 experiment: https://yjernite.github.io/lfqa.html)\n# (replace special tokens (till nothing left) from training data first, then format the form.)\n# the other is to export the hidden states, use fixed linear networks and get reward score.\n\n# but reward modeling is multifold. we can use multiqa-bert, retribert, comparing prompts to answers.\n# for sentence transformers, you can pass similarity score in dataset.\n\n# train a multi-label model from autoregressive model using \"AutoModelForSequenceClassification\"\n# (as hidden states to our linear neural network, as reward model)\n# you can also use \"SetFit\" to train a level of labels based on existing sentence transformers\n# (like truthfulness from 1 to 7, informative and so on.)\n\n# from my point of view, using such large language model is not memory efficient\n# compared to retrieval+generative based QA model (ILQA), which can also be fine-tuned.\n# somebody using RETRO is saying the same thing.\n# anyway, both approaches require a proper reward function/model or metric!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I think WebGPT, RETRO and LFQA have commondities. They all use references.\n\n# WebGPT mimics actions taken during web browsing\n# RETRO uses FAISS as backend\n# LFQA combines retrieval BERT with generative BART","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so let's begin investigation.\n# give prompt and compare which is better?\n# to predict logits, (decide is A or B) see: https://github.com/LouisCastricato/limited-data-scaling-laws/blob/79fd6fcb452d9b513d9520c144a447d5a11ccecd/critic_models.py#L55\n\nspecial_token = \"<P>\" # hey you make sure this exists in the tokenizer, since it differs with model\n\ndef replaceTillNothingLeft(string:str, objective:str,target:str=\"\"):\n    while objective in string:\n        string = string.replace(objective,target)\n    return string\n\nRTNL = lambda x: replaceTillNothingLeft(x,special_token) # shame we cannot typehint you! is it?\nquestion = \"What is the most beautiful thing in this world?\"\nans_0 = \"Dog.\"\nans_1 = \"Cat.\"\nmprompt = f\"Given the question and two answers, find the better answer.{special_token}Question: {RTNL(question)}{special_token}A: {RTNL(ans_0)}{special_token}B: {RTNL(ans_1)}{special_token}Mark it as A or B.\"\n\n# alright, we know the drill. leave it for now.\nmprompt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diag_0 =  lambda spec_token: f\"question: {question}{spec_token}answer: {ans_0}\"\ndiag_1 =  lambda spec_token: f\"question: {question}{spec_token}answer: {ans_1}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now export hidden states.\n# we know \"roberta\" \"gpt-like\" and \"FLAN-T5\" (no example!)\n\n# first, roberta. (seen in ILQL: https://github.com/Sea-Snell/Implicit-Language-Q-Learning/blob/0ee79b7cee11a56c20ecc54ad246c7033af0ee92/src/toxicity/reward_model.py)\n# bert-like models seems have fixed hidden size?\n\n# you want changes? it has special \"attention mask\" and pooling output.\n\ndef strip_from_end(str_item, strip_key):\n    return strip_from_beginning(str_item[::-1], strip_key[::-1])[::-1]\n\ndef strip_from_beginning(str_item, strip_key):\n    if str_item[:len(strip_key)] == strip_key:\n        return str_item[len(strip_key):]\n    return str_item\n\nitems = {}\nspec_token = \"</s>\" # what is this token?\nraw_strs = [diag_0(spec_token),diag_1(spec_token)] # what should be in this thing?\n\n\nfrom transformers import RobertaModel, RobertaTokenizer\n\nmodel_id = \"deepset/roberta-base-squad2\" # whatever.\n\nmodel = RobertaModel.from_pretrained(model_id)\nroberta_tokenizer = RobertaTokenizer.from_pretrained(model_id)\n\nimport torch\n\n# use cpu instead?\ndevice = \"cpu\"\n\n# whatever. let's just input whatever we want.\n# raw_strs = [strip_from_end(strip_from_beginning((raw_str[:raw_str.find('<|pad|>')] if raw_str.find('<|pad|>') != -1 else raw_str).strip(), '<a>'), '</a> </eod>').strip() for raw_str in raw_strs]\nnew_tokenized = roberta_tokenizer(raw_strs, padding=True)\nitems['tokens'] = torch.tensor(new_tokenized['input_ids']).to(device)\nitems['attn_mask'] = torch.tensor(new_tokenized['attention_mask']).to(device)\n# items\nroberta_tokenizer.sep_token # yeah we've got it.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.hidden_size # sure. but how do we decompose this model?","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i do agree now, gpu is only for acceleration.\n# if with enough RAM we can handle large model, for test purpose.\n# now get the embedding\npooler_output = model(items['tokens'],attention_mask = items['attn_mask']).pooler_output\npooler_output, pooler_output.shape\n\n# and now you can map this thing to reward. really?\n# they are similar! damn.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install opendelta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now, for gpt-like models, how to apply this?\n# to show model structures, just print it\n# seen in here: https://github.com/James4Ever0/limited-data-scaling-laws/blob/main/train_rm/train_rm.py\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nname = \"EleutherAI/gpt-neo-125M\"\nmodel = AutoModelForCausalLM.from_pretrained(name)\n# one crucial step: sending \"transformer\" in\ntransformer = model.transformer\ntokenizer = AutoTokenizer.from_pretrained(name)\n\n# if you want to use special tokens in gpt, you have to resize the model head.\n# https://github.com/huggingface/transformers/issues/8039\n# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n# tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n# tokenizer.add_special_tokens({'pad_token': ' '}) # swapped. fuck!","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:24:06.552316Z","iopub.execute_input":"2022-12-25T16:24:06.553792Z","iopub.status.idle":"2022-12-25T16:24:34.140873Z","shell.execute_reply.started":"2022-12-25T16:24:06.553655Z","shell.execute_reply":"2022-12-25T16:24:34.139600Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"944cdc4b02294564bb7c10af4885977d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8433b0d8abd349d2bf6b337d97df00cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6e674cee1e14869847539e3b42d4237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef20b6f69c33413d850f068259e17a4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d94a4008fe474abac9ae9c836721a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543e5ab3a0594c81a71ecdb1cf00d999"}},"metadata":{}}]},{"cell_type":"code","source":"# tokenizer.add_special_tokens({'sep_token': '[SEP]'}) # i guess this shit is to be blamed.\n# since gpt-neo use only 50257 features. fuck.\n# transformer","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:24:52.412303Z","iopub.execute_input":"2022-12-25T16:24:52.413095Z","iopub.status.idle":"2022-12-25T16:24:52.427284Z","shell.execute_reply.started":"2022-12-25T16:24:52.413051Z","shell.execute_reply":"2022-12-25T16:24:52.425607Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"GPTNeoModel(\n  (wte): Embedding(50257, 768)\n  (wpe): Embedding(2048, 768)\n  (drop): Dropout(p=0.0, inplace=False)\n  (h): ModuleList(\n    (0): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (1): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (2): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (3): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (4): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (5): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (6): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (7): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (8): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (9): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (10): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (11): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# dir(tokenizer)\n[tokenizer.sep_token_id,tokenizer.pad_token,tokenizer.pad_token_id] # i guess this is new to the model huh?\n# no sep_token?\n# it does have place.\n# [(x,tokenizer.__dict__.get(x,None)) for x in dir(tokenizer)]\n# can you add more?","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:25:15.264073Z","iopub.execute_input":"2022-12-25T16:25:15.265052Z","iopub.status.idle":"2022-12-25T16:25:15.276697Z","shell.execute_reply.started":"2022-12-25T16:25:15.265004Z","shell.execute_reply":"2022-12-25T16:25:15.274829Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Using pad_token, but it is not set yet.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[None, None, None]"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode([0,50254,50255,50256])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.encode(\" \") # space: 220, as replacement of padding?\ntokenizer.encode(\".\")  # period: 13","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n# model(torch.tensor([[50256]])) # starting from 50257 this shit cannot decode. what to do? just use attention mask.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's encode\n# spec_token = tokenizer.sep_token\n# replace this token with others!\nspec_token = \";\" #no such token you fuck.\n# spec_token = \"[SEP]\" # is it still avaliable?\n# use it anyway.\n\nquestion = \"What is the most beautiful thing in this world?\"\nans_0 = \"Dog.\"\nans_1 = \"Cat.\"\n\ntokenizer.pad_token_id = 220\n# tokenizer.pad_token = \" \"\n# tokenizer.sep_token = \";\"\n# using reserved utf-8 codes? i don't think so.\n# print(\"PAD TOKEN?\",tokenizer.pad_token)\n# print(\"SEP TOKEN?\", tokenizer.sep_token)\n# now it is using endoftext for padding. interesting?\n# great?\n\n# still, missing something.\n# string = [f'Query: {question}{spec_token}Answer: {answer}' for answer in [ans_0, ans_1]]\nstring = [f'Query: {question}{spec_token}Answer: {answer}' for answer in [ans_0, ans_1,\"Frog and Cat.\"]]\nprint(\"STRING?\",string)\n# you must use padding. fuck.\nout = tokenizer(string, return_tensors=\"pt\", padding=True, truncation=False) # use padding here?\nout # do not use endoftext","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:25:40.274225Z","iopub.execute_input":"2022-12-25T16:25:40.274665Z","iopub.status.idle":"2022-12-25T16:25:40.297645Z","shell.execute_reply.started":"2022-12-25T16:25:40.274631Z","shell.execute_reply":"2022-12-25T16:25:40.296782Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"STRING? ['Query: What is the most beautiful thing in this world?;Answer: Dog.', 'Query: What is the most beautiful thing in this world?;Answer: Cat.', 'Query: What is the most beautiful thing in this world?;Answer: Frog and Cat.']\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[20746,    25,  1867,   318,   262,   749,  4950,  1517,   287,   428,\n           995,    30,    26, 33706,    25,  8532,    13,   220,   220],\n        [20746,    25,  1867,   318,   262,   749,  4950,  1517,   287,   428,\n           995,    30,    26, 33706,    25,  5181,    13,   220,   220],\n        [20746,    25,  1867,   318,   262,   749,  4950,  1517,   287,   428,\n           995,    30,    26, 33706,    25, 28328,   290,  5181,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"# check fidality.\ntokenizer.decode([20746,    25,  1867,   318,   262,   749,  4950,  1517,   287,   428,\n           995,    30,    26, 33706,    25,  8532,    13, 50256, 50256])","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:21:19.404993Z","iopub.execute_input":"2022-12-25T16:21:19.405478Z","iopub.status.idle":"2022-12-25T16:21:26.470094Z","shell.execute_reply.started":"2022-12-25T16:21:19.405437Z","shell.execute_reply":"2022-12-25T16:21:26.468796Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'Query: What is the most beautiful thing in this world?;Answer: Dog.<|endoftext|><|endoftext|>'"},"metadata":{}}]},{"cell_type":"code","source":"# what is token 13?\ntokenizer.decode([13]) # period. fuck.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_size = model.config.hidden_size\nhidden_size","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:27:53.103921Z","iopub.execute_input":"2022-12-25T16:27:53.104401Z","iopub.status.idle":"2022-12-25T16:27:53.111866Z","shell.execute_reply.started":"2022-12-25T16:27:53.104366Z","shell.execute_reply":"2022-12-25T16:27:53.110674Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"768"},"metadata":{}}]},{"cell_type":"code","source":"from torch import nn\nv_head = nn.Linear(hidden_size, 1, bias=False)\n# model\n# inspect model structure?","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:27:55.284080Z","iopub.execute_input":"2022-12-25T16:27:55.284480Z","iopub.status.idle":"2022-12-25T16:27:55.291705Z","shell.execute_reply.started":"2022-12-25T16:27:55.284449Z","shell.execute_reply":"2022-12-25T16:27:55.290306Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# let's get hidden states.\ninput_ids = out['input_ids']\nattention_mask = out['attention_mask']\n\nprint(\"INPUT?\",input_ids)\nprint(\"trying\")\n\n# print(dir(model))\noutput = transformer( \n            input_ids,\n            attention_mask=attention_mask,\n#     output_hidden_states=True\n        )\n\n# out of range?\n# transformer_outputs\n# no hidden states this time.\n# print(len(transformer_outputs)) #2\nhidden_states = output[0]\nhidden_states.shape\n# print(\"output_1?\",len(transformer_outputs[1]))\n# hidden_states.shape # strange. # why the fuck?\n# print(dir(transformer_outputs)) # what the fuck is going on?\n# print(len(transformer_outputs.hidden_states)) # None? 13 hidden states? wtf?\n# #size: 17,768 (original)\n# torch.Size([3, 19, 768]) fucking A!\n# torch.Size([3, 19, 50257])\n# calculate average score?\n# input_ids, input_ids.shape # 19 now?\n\n# attention_mask, attention_mask.shape\n# this attention mask, is suitable for directing attention to non \"pad\" or \"section\" tokens","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:27:14.386713Z","iopub.execute_input":"2022-12-25T16:27:14.387114Z","iopub.status.idle":"2022-12-25T16:27:14.576697Z","shell.execute_reply.started":"2022-12-25T16:27:14.387084Z","shell.execute_reply":"2022-12-25T16:27:14.575494Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"INPUT? tensor([[20746,    25,  1867,   318,   262,   749,  4950,  1517,   287,   428,\n           995,    30,    26, 33706,    25,  8532,    13,   220,   220],\n        [20746,    25,  1867,   318,   262,   749,  4950,  1517,   287,   428,\n           995,    30,    26, 33706,    25,  5181,    13,   220,   220],\n        [20746,    25,  1867,   318,   262,   749,  4950,  1517,   287,   428,\n           995,    30,    26, 33706,    25, 28328,   290,  5181,    13]])\ntrying\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 19, 768])"},"metadata":{}}]},{"cell_type":"code","source":"model.config.hidden_size","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:27:19.819829Z","iopub.execute_input":"2022-12-25T16:27:19.820338Z","iopub.status.idle":"2022-12-25T16:27:19.828842Z","shell.execute_reply.started":"2022-12-25T16:27:19.820299Z","shell.execute_reply":"2022-12-25T16:27:19.827639Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"768"},"metadata":{}}]},{"cell_type":"code","source":"r = v_head(hidden_states)\nr.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:28:05.151981Z","iopub.execute_input":"2022-12-25T16:28:05.152481Z","iopub.status.idle":"2022-12-25T16:28:05.160181Z","shell.execute_reply.started":"2022-12-25T16:28:05.152443Z","shell.execute_reply":"2022-12-25T16:28:05.159250Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 19, 1])"},"metadata":{}}]},{"cell_type":"code","source":"rwList = r.squeeze(-1)\nrwList.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:28:35.314132Z","iopub.execute_input":"2022-12-25T16:28:35.314583Z","iopub.status.idle":"2022-12-25T16:28:35.323596Z","shell.execute_reply.started":"2022-12-25T16:28:35.314548Z","shell.execute_reply":"2022-12-25T16:28:35.322165Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 19])"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nrw_mean = torch.mean(rwList)\nrw_mean\n# great. what now?","metadata":{"execution":{"iopub.status.busy":"2022-12-25T16:29:17.228835Z","iopub.execute_input":"2022-12-25T16:29:17.229228Z","iopub.status.idle":"2022-12-25T16:29:17.239073Z","shell.execute_reply.started":"2022-12-25T16:29:17.229181Z","shell.execute_reply":"2022-12-25T16:29:17.238147Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor(0.0589, grad_fn=<MeanBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"# saying you should install \"apex\" if you want accelerated T5\n# from document?\n# apex is provided by nvidia: https://github.com/NVIDIA/apex\n\n# installation:\n\n# git clone https://github.com/NVIDIA/apex\n# cd apex\n# pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n\n# or:\n\n# pip install git+https://github.com/NVIDIA/apex.git","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# you tune FLAN-T5 for this job?\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel_id = \"google/flan-t5-base\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T18:35:08.909948Z","iopub.execute_input":"2022-12-25T18:35:08.910576Z","iopub.status.idle":"2022-12-25T18:35:42.610501Z","shell.execute_reply.started":"2022-12-25T18:35:08.910530Z","shell.execute_reply":"2022-12-25T18:35:42.609353Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"055cc28dd08542ba88c93cdbb0a9c27f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/945M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ed10d9452b4900b4f3214baccbc243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ea7a59c6584967898bd69b21d37a1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e74508f31c4dc4a78bdfbb7f977fd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df9c53fe9364b22aab99870dd84286e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f7104513b448a18d1cd810e160f950"}},"metadata":{}}]},{"cell_type":"code","source":"# dir(tokenizer)\n# tokenizer.all_special_tokens\ntokenizer.eos_token, tokenizer.pad_token","metadata":{"execution":{"iopub.status.busy":"2022-12-25T18:41:40.377687Z","iopub.execute_input":"2022-12-25T18:41:40.378143Z","iopub.status.idle":"2022-12-25T18:41:40.386698Z","shell.execute_reply.started":"2022-12-25T18:41:40.378110Z","shell.execute_reply":"2022-12-25T18:41:40.385736Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"('</s>', '<pad>')"},"metadata":{}}]},{"cell_type":"code","source":"# dir(model.config)\n# nothing here!\ntoken_ids = (model.config.sep_token_id,model.config.pad_token_id) # it does have \nprint(token_ids)\nminput_0 = \"Question: What is the most beautiful animal in this world? A: Dog. B: Cat. Answer either A or B:\"\nminput_1 = \"Question: What is the most beautiful animal in this world? A: Dog. B: Frog. Answer either A or B:\"\nminputs = [minput_0,minput_1] # no padding? fuck?\nmemb_input = tokenizer(minputs,return_tensors=\"pt\",padding=True)\nprint(\"INPUT VECTOR?\",memb_input)\n# output = model(input_ids = memb_input['input_ids'])\ndecoder_input_ids = torch.ones((memb_input['input_ids'].shape[0], 1), dtype=torch.long) * model.config.decoder_start_token_id\noutput = model(input_ids = memb_input['input_ids'],attention_mask=memb_input['attention_mask'],decoder_input_ids=decoder_input_ids)\n# print(\"OUTPUT?\",output)\n# great. seen in openai.\nhidden_params = output[2]\nhidden_params","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:20:28.163135Z","iopub.execute_input":"2022-12-25T21:20:28.163540Z","iopub.status.idle":"2022-12-25T21:20:28.507407Z","shell.execute_reply.started":"2022-12-25T21:20:28.163510Z","shell.execute_reply":"2022-12-25T21:20:28.506102Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"(None, 0)\nINPUT VECTOR? {'input_ids': tensor([[11860,    10,   363,    19,     8,   167,   786,  2586,    16,    48,\n           296,    58,    71,    10,  6751,     5,   272,    10,  3431,     5,\n         11801,   893,    71,    42,   272,    10,     1,     0],\n        [11860,    10,   363,    19,     8,   167,   786,  2586,    16,    48,\n           296,    58,    71,    10,  6751,     5,   272,    10,   377,  3822,\n             5, 11801,   893,    71,    42,   272,    10,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1]])}\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 1.4192e-02,  2.1341e-01, -2.0558e-01,  ..., -3.9713e-02,\n          -6.6147e-02, -4.7994e-02],\n         [ 6.8610e-03,  7.3397e-02, -4.6221e-02,  ..., -7.0633e-02,\n          -9.5900e-03, -9.7069e-02],\n         [ 9.3572e-02,  2.8400e-02, -5.0034e-02,  ...,  6.7745e-02,\n           5.5376e-02,  8.1821e-02],\n         ...,\n         [ 3.3754e-02, -4.9938e-03, -2.8832e-02,  ...,  5.8913e-02,\n           2.2033e-01,  1.1926e-01],\n         [ 9.4769e-03,  6.4210e-03,  1.4606e-02,  ...,  1.4873e-03,\n          -2.6005e-03,  8.3175e-04],\n         [-2.8552e-01,  1.2683e-01,  6.7264e-02,  ...,  2.2612e-01,\n           7.2569e-02,  1.8119e-01]],\n\n        [[ 9.6625e-03,  1.9815e-01, -1.9542e-01,  ..., -4.1598e-02,\n          -6.1196e-02, -4.0653e-02],\n         [ 1.7699e-04,  5.5468e-02, -2.8853e-02,  ..., -8.6591e-02,\n          -1.3577e-02, -8.6231e-02],\n         [ 7.5097e-02,  1.6171e-02, -5.4481e-02,  ...,  5.4896e-02,\n           4.7474e-02,  9.7275e-02],\n         ...,\n         [ 1.6699e-01, -6.7642e-03,  2.5620e-02,  ..., -3.7159e-01,\n           2.8298e-02,  2.2253e-01],\n         [ 3.5454e-02, -5.5676e-03, -2.9649e-02,  ...,  5.3261e-02,\n           2.2866e-01,  1.1659e-01],\n         [ 9.4553e-03,  6.5513e-03,  1.4637e-02,  ...,  1.4343e-03,\n          -2.5987e-03,  8.2390e-04]]], grad_fn=<MulBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"val = v_head(hidden_params).squeeze(-1)\nprint(\"VAL_SPARSE:\",val.shape)\nval_sum = torch.sum(val)\n# you can use some non-linear magic.\nprint(val_sum)","metadata":{"execution":{"iopub.status.busy":"2022-12-25T21:20:34.585094Z","iopub.execute_input":"2022-12-25T21:20:34.585590Z","iopub.status.idle":"2022-12-25T21:20:34.597519Z","shell.execute_reply.started":"2022-12-25T21:20:34.585552Z","shell.execute_reply":"2022-12-25T21:20:34.595487Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"VAL_SPARSE: torch.Size([2, 28])\ntensor(-2.2909, grad_fn=<SumBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's load sentence transformers.\n# find similar query/answer pairs?\n# if you just want to use this \"retrival\" based model without training, you are doomed.\n# retrival based model is for building train sets. is it?\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer","metadata":{"execution":{"iopub.status.busy":"2022-12-25T22:02:26.838720Z","iopub.execute_input":"2022-12-25T22:02:26.839411Z","iopub.status.idle":"2022-12-25T22:02:45.131839Z","shell.execute_reply.started":"2022-12-25T22:02:26.839372Z","shell.execute_reply":"2022-12-25T22:02:45.130511Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m916.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.20.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.11.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.7)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.10.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=001ee7ea8059834f57882fca84b0514437fbc33baf6099c8e356d9f777fb0cbe\n  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# model_id = \"yjernite/retribert-base-uncased\" # working or not?\nmodel_id = \"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\" # oh shit. train it first?\nfrom sentence_transformers import CrossEncoder\nst = CrossEncoder(model_id,num_labels=1) # for training?\n# st = SentenceTransformer(model_id)\nst","metadata":{"execution":{"iopub.status.busy":"2022-12-25T22:13:16.304040Z","iopub.execute_input":"2022-12-25T22:13:16.304862Z","iopub.status.idle":"2022-12-25T22:13:35.566249Z","shell.execute_reply.started":"2022-12-25T22:13:16.304821Z","shell.execute_reply":"2022-12-25T22:13:35.565076Z"},"trusted":true},"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/891 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67298b317aaf46709566835c5bcae3c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/449M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eff09fa0db749a5800906554519e288"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d630961a53de4c22af10ccc295aaa465"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6530e636c5294d8cab95f4876fbd1f0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d04f255dec404995a185f552aa88ad27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa5a7e3359446f0b7ee3638aa003dc9"}},"metadata":{}},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"<sentence_transformers.cross_encoder.CrossEncoder.CrossEncoder at 0x7fd58aa76a10>"},"metadata":{}}]},{"cell_type":"code","source":"# now let's compare.\nquery = \"What is the most beautiful animal in this world?\"\n# ans_0 = \"Samoyed.\" # better than dog!\nans_0 = \"Dog.\"\nans_1 = \"Cat.\"\nmdata = [[query, ans] for ans in [ans_0, ans_1]]\nscores = st.predict(mdata)\nscores # so it prefers cat.","metadata":{"execution":{"iopub.status.busy":"2022-12-25T23:03:08.990327Z","iopub.execute_input":"2022-12-25T23:03:08.990942Z","iopub.status.idle":"2022-12-25T23:03:09.078772Z","shell.execute_reply.started":"2022-12-25T23:03:08.990899Z","shell.execute_reply":"2022-12-25T23:03:09.077555Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da63f5cd7a1e478884281abba9490702"}},"metadata":{}},{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"array([0.19621429, 0.68443924], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# let's train this shit to prefer do      g.\n# i think bi-encoders (retrieval) based are more memory efficient? or not?\nfrom sentence_transformers import InputExample # you can use discrete grades.\n# this is asymmetric. query and answer.\ntrain_samples = [\n  InputExample(texts=[query, ans_0], label=0.8), # ugly cat!\n  InputExample(texts=[query, ans_1], label=0.1),\n]\ntrain_batch_size = 1 # you can raise this thing.\nnum_epochs = 20\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n\nimport math\ntrain_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n\n# warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\nwarmup_steps = 0\n\n# use the same sample?\n# We add an evaluator, which evaluates the performance during training\nevaluator = CECorrelationEvaluator.from_input_examples(train_samples, name='sts-dev')\n\n# We wrap train_samples (which is a List[InputExample]) into a pytorch DataLoader\n\nmodel_save_path = \"./model_qa\"\n\n# Train the model\n# wtf? you cannot fit this shit?\n# the name is incorrect.\nst.fit(train_dataloader=train_dataloader,\n          evaluator=evaluator,\n          epochs=num_epochs,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path)\n\nprint(\"TRAIN COMPLETE\")","metadata":{"execution":{"iopub.status.busy":"2022-12-25T23:50:24.601470Z","iopub.execute_input":"2022-12-25T23:50:24.601899Z","iopub.status.idle":"2022-12-25T23:51:27.336144Z","shell.execute_reply.started":"2022-12-25T23:50:24.601864Z","shell.execute_reply":"2022-12-25T23:51:27.334965Z"},"trusted":true},"execution_count":75,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5039e57b6b943efb1c833977c8c6e6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b70863fc8e94721a83dee0468725f12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9a51f0fe504f5283a296b2de214d3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29e19e75dd1042fbbb006b5e72c112ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa1bf0d66ca417aa37fef4535329352"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a1716002280426e95225171661dadd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d190b2218a18493b90c8d05939477d13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd2e41404a848e4a536c3fd7480cede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4423e9f4595349459eb56bffbbb12384"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02c5ab4c81041edb69f38770eb11dd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87d776ce3d7d48e49175e42d30f209e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369b6a63dcf54ce1b423a2da06de551e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"782020983e4e4438990477d4a7337c06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38e36a2afc34b6787a36fcdeca895e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24958b975895476dae2d91564f74955b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0b50d411789448186851ec0f51891d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d80d2f45ef4ae385750ef16c638b4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"692f4b87459e458e95b72ff37ddc2772"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b8a0edbe964a8ba7d720875a1e5d83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d1e88c522d4c7eb9d2fef061030c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa6f2b5879440bf9055198e8a73d25e"}},"metadata":{}},{"name":"stdout","text":"TRAIN COMPLETE\n","output_type":"stream"}]},{"cell_type":"code","source":"scores = st.predict(mdata)# how can you have >1 things?\nscores # fuck. \n# array([ 1.9676722, -2.1964266], dtype=float32)\n# this is how you train shits? anyway, it does learn some human preference.","metadata":{"execution":{"iopub.status.busy":"2022-12-27T10:05:54.350251Z","iopub.execute_input":"2022-12-27T10:05:54.351181Z","iopub.status.idle":"2022-12-27T10:05:54.446477Z","shell.execute_reply.started":"2022-12-27T10:05:54.351063Z","shell.execute_reply":"2022-12-27T10:05:54.444552Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3161990796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# how can you have >1 things?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;31m# fuck.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# array([ 1.9676722, -2.1964266], dtype=float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# this is how you train shits? anyway, it does learn some human preference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"],"ename":"NameError","evalue":"name 'st' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# load another model, be it multilingual QA\n# oops nothing found on sentence bert.\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n# loading your own model?\n# note: do not fine-tune that multi-language sentence similarity model since that will break shit.\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\" # just load english shit.\n# guide on how to make models multilingual:\n# https://www.sbert.net/examples/training/multilingual/README.html\nst = SentenceTransformer(model_id)","metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:56:02.762685Z","iopub.execute_input":"2022-12-29T12:56:02.763168Z","iopub.status.idle":"2022-12-29T12:56:42.083049Z","shell.execute_reply.started":"2022-12-29T12:56:02.763074Z","shell.execute_reply":"2022-12-29T12:56:42.081611Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m613.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.20.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.64.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.11.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.12.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.7)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.10.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (8.0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=ec720d91a9313d959d95c14746314754019b3eae3396cb04c330cbf56191f482\n  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76678ccb463c4e76817638bcd3003708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1141b062e1e344bf9c35c63f3045b2e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72640e2314bd4559842e1a0230624d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df7d8935f75d42d99f7fcfa9ce9b4654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9dd97905f864695b15d272128581a04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a214a4e44824270a09fe90cd36c5737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972191549d754ac8bcf41f9e6ddfe994"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbb0e313d88d4c58a412981b7d824317"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ca63cf0e4cc4ce7b7f1d0c9785da769"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8cf105f1c38414bb5ab59df0ec87ac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12efe84eb597461885fd3a148aa7b573"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604ec9c757b24aa6a2c006a3c643fbb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d65b6169f0436188d4beddbb787d75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd096c81e254cc7a772bd1d1e252f28"}},"metadata":{}}]},{"cell_type":"code","source":"# st\nfrom sentence_transformers import util\n\n\ndef testsim():\n    # now let's compare.\n    query = \"What is the most beautiful animal in this world?\"\n    # ans_0 = \"Samoyed.\" # better than dog!\n    ans_0 = \"Dog.\"\n    ans_1 = \"Cat.\"\n    query_embedding = st.encode(query)\n\n    passage_embedding = st.encode([ans_0, ans_1])\n\n    print(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding))# shit?\n\ntestsim()\n","metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:57:26.378969Z","iopub.execute_input":"2022-12-29T12:57:26.380035Z","iopub.status.idle":"2022-12-29T12:57:26.577637Z","shell.execute_reply.started":"2022-12-29T12:57:26.379991Z","shell.execute_reply":"2022-12-29T12:57:26.576608Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a10ef18b6b4d59ba44cc2a1928b4a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28de5508d944766a2d97c71adff5788"}},"metadata":{}},{"name":"stdout","text":"Similarity: tensor([[0.3492, 0.3051]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# again, how to train?\n\nfrom sentence_transformers import InputExample, losses\nfrom torch.utils.data import DataLoader               \n\ntrain_examples = [InputExample(texts=[query, ans_0], label=0.8),\n   InputExample(texts=[query, ans_1], label=0.3)]\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=2)\n\ntrain_loss = losses.CosineSimilarityLoss(model)\n\n#Tune the model\nst.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\nprint(\"TRAINING COMPLETE.\")\n# then you must test the result.","metadata":{"execution":{"iopub.status.busy":"2022-12-29T12:57:32.505494Z","iopub.execute_input":"2022-12-29T12:57:32.505900Z","iopub.status.idle":"2022-12-29T12:57:32.777168Z","shell.execute_reply.started":"2022-12-29T12:57:32.505865Z","shell.execute_reply":"2022-12-29T12:57:32.775516Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3320723896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m train_examples = [InputExample(texts=[query, ans_0], label=0.8),\n\u001b[0m\u001b[1;32m      7\u001b[0m    InputExample(texts=[query, ans_1], label=0.3)]\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"],"ename":"NameError","evalue":"name 'query' is not defined","output_type":"error"}]},{"cell_type":"code","source":"testsim()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use this.\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# i think it should not be too different. let's make it binary.\n\nmodel_id = \"EleutherAI/gpt-neo-125M\"\n\nnum_labels = 1  # etc.\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id,\n    num_labels=num_labels,\n    problem_type=\"multi_label_classification\",  # this is important\n                                                          )\nprint(\"model loaded.\")\n\n# you should train this model.","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:42:26.495530Z","iopub.execute_input":"2022-12-30T21:42:26.496179Z","iopub.status.idle":"2022-12-30T21:43:24.221110Z","shell.execute_reply.started":"2022-12-30T21:42:26.496089Z","shell.execute_reply":"2022-12-30T21:43:24.220115Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14111f8f175e4910bf4fac2e3eb4e045"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4661018b74409b8f9dbdbf67eef979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f42f09f2868641eb85370f5485acc4e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f52e96b6bbd4e5782a333f9ef591f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a783d91a10471cb76c415d6c48750c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9ed7226e024046be88cf2aecbeee06"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['transformer.h.3.attn.attention.bias', 'score.weight', 'transformer.h.9.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.5.attn.attention.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"model loaded.\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n\n# tutorial: \n# handle the damn missing special token problem for causal LM:\n# https://github.com/huggingface/transformers/issues/2630\n\n# this is bullshit. but it is needed! we just need to remap and mask it\n# tokenizer.pad_token_id\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# in order to get a proper view among all \"affected\" tokens after change:\ntokenizer.pad_token, tokenizer.pad_token_id, tokenizer.eos_token, tokenizer.eos_token_id\n# the same? we are good.","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:43:57.742732Z","iopub.execute_input":"2022-12-30T21:43:57.744372Z","iopub.status.idle":"2022-12-30T21:43:57.754765Z","shell.execute_reply.started":"2022-12-30T21:43:57.744289Z","shell.execute_reply":"2022-12-30T21:43:57.753854Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"('<|endoftext|>', 50256, '<|endoftext|>', 50256)"},"metadata":{}}]},{"cell_type":"code","source":"# dir(model.config)\n# None? Let's set it.\nmodel.config.pad_token_id=tokenizer.pad_token_id # useless fuck.\nmodel.config.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:44:04.110040Z","iopub.execute_input":"2022-12-30T21:44:04.110502Z","iopub.status.idle":"2022-12-30T21:44:04.117904Z","shell.execute_reply.started":"2022-12-30T21:44:04.110453Z","shell.execute_reply":"2022-12-30T21:44:04.116776Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"50256"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:44:10.006535Z","iopub.execute_input":"2022-12-30T21:44:10.007121Z","iopub.status.idle":"2022-12-30T21:44:10.014316Z","shell.execute_reply.started":"2022-12-30T21:44:10.007091Z","shell.execute_reply":"2022-12-30T21:44:10.013314Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}]},{"cell_type":"code","source":"# now encode this.\ndata = [\"sentence a: aaa\", \"sentence b: bb super b\"] # wrong?\n\ntokenizer.padding_side=\"left\" # important!\n\ninput_data = tokenizer(data,padding=True,return_tensors=\"pt\") # set padding to left. mask these fucks.\ninput_data","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:44:12.701297Z","iopub.execute_input":"2022-12-30T21:44:12.702182Z","iopub.status.idle":"2022-12-30T21:44:12.722300Z","shell.execute_reply.started":"2022-12-30T21:44:12.702130Z","shell.execute_reply":"2022-12-30T21:44:12.721115Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[50256, 50256, 34086,   594,   257,    25,   257,  7252],\n        [34086,   594,   275,    25,   275,    65,  2208,   275]]), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"# let's input.\n# https://www.alexanderjunge.net/blog/til-multi-label-automodelforsequenceclassification/\n\noutput = model(input_data['input_ids'],\n      # yeah you of course see the damn difference.\n      attention_mask = input_data['attention_mask'] # and you must be fucking kidding me.\n      # transformers/src/transformers/models/t5/modeling_t5.py\n      # https://cocomanga.xyz/comic/45241/650372.html#p=5\n     )[0] # yeah you've got it, but where's the fucking mask?\n# ,attn_mask=input_data['attention_mask']) # still not fucking working. not allowing padding shit.\n\n# digits are not good. how to calculate loss and backpropagate?\n# just omit?\noutput, output.dtype","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:45:39.725258Z","iopub.execute_input":"2022-12-30T21:45:39.726007Z","iopub.status.idle":"2022-12-30T21:45:39.797854Z","shell.execute_reply.started":"2022-12-30T21:45:39.725971Z","shell.execute_reply":"2022-12-30T21:45:39.797007Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(tensor([[-40.5820],\n         [-39.3169]], grad_fn=<IndexBackward0>),\n torch.float32)"},"metadata":{}}]},{"cell_type":"code","source":"# to make it trainable?\n\nimport torch\n\n# the bot prefers \"unsqueeze\"\n# labels = torch.tensor([[0],[1]]) # shall be float32.\n# labels = torch.tensor([float(0),float(1)]).unsqueeze(0) # 0? are you fucking sure? shape mismatch?\nlabels = torch.tensor([float(0),float(1)]).unsqueeze(-1) # 0? are you fucking sure? shape mismatch?\nlabels, labels.dtype # match? but wtf?\n\n# note: nvidia/apex is not compatible below Turing architecture.","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:48:22.682097Z","iopub.execute_input":"2022-12-30T21:48:22.682856Z","iopub.status.idle":"2022-12-30T21:48:22.690841Z","shell.execute_reply.started":"2022-12-30T21:48:22.682819Z","shell.execute_reply":"2022-12-30T21:48:22.689890Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(tensor([[0.],\n         [1.]]),\n torch.float32)"},"metadata":{}}]},{"cell_type":"code","source":"# now let's deal with the loss.\n# to make it trainable?\n\nimport torch\n\n# the bot prefers \"unsqueeze\"\n# labels = torch.tensor([[0],[1]]) # shall be float32.\n# labels = torch.tensor([float(0),float(1)]).unsqueeze(0) # 0? are you fucking sure? shape mismatch?\nlabels = torch.tensor([float(0),float(1)]).unsqueeze(-1) # 0? are you fucking sure? shape mismatch?\nlabels, labels.dtype # match? but wtf?\n\n# from torch.nn import CrossEntropyLoss # loss not good?\n# from torch.nn import MSELoss\nfrom torch.nn import L1Loss\n# from torch.nn import L2Loss\n# from torch.nn import NLLLoss\nfrom transformers import AdamW\n\n# optimizer = AdamW(model.parameters(), lr=1) # too high? just for demo.\n# this is bullshit.\n# optimizer = AdamW(model.parameters(), lr=0.02) # too high? just for demo.\noptimizer = AdamW(model.parameters(), lr=2e-5) # too low?\n# loss_fn = MSELoss()\nloss_fn = L1Loss()\n# loss_fn = CrossEntropyLoss()\n# loss_fn = NLLLoss()\n\n\n# alter output?\n\noutput = model(input_data['input_ids'],\n      # yeah you of course see the damn difference.\n      attention_mask = input_data['attention_mask'] # and you must be fucking kidding me.\n      # transformers/src/transformers/models/t5/modeling_t5.py\n      # https://cocomanga.xyz/comic/45241/650372.html#p=5\n)[0]\n\nlabels = torch.tensor([float(0),float(1)])\n\n\nfor _ in range(10000): # simulate training\n    output = output.reshape(-1)\n#     labels = labels.unsqueeze(0)\n\n    # it selects moutput[0] as output.\n    print(\"OUTPUT?\",output,output.dtype)\n    print(\"LABELS?\",labels,labels.dtype)\n    loss = loss_fn(output, labels) # no loss? wtf?\n    print('LOSS?',loss)\n    # Backward pass\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    labels = torch.tensor([float(0),float(1)])\n    output = model(input_data['input_ids'],\n      # yeah you of course see the damn difference.\n      attention_mask = input_data['attention_mask'] # and you must be fucking kidding me.\n      # transformers/src/transformers/models/t5/modeling_t5.py\n      # https://cocomanga.xyz/comic/45241/650372.html#p=5\n    )[0]\n    \n\nprint(\"MODEL TRAINED.\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:57:10.402925Z","iopub.execute_input":"2022-12-30T21:57:10.403820Z","iopub.status.idle":"2022-12-30T21:58:49.345482Z","shell.execute_reply.started":"2022-12-30T21:57:10.403776Z","shell.execute_reply":"2022-12-30T21:58:49.344146Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"OUTPUT? tensor([  96.7552, -101.6096], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(99.6824, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([  96.8062, -101.2893], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(99.5477, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([  96.8513, -101.0086], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(99.4300, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([  96.8945, -100.7339], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(99.3142, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([  96.9399, -100.4571], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(99.1985, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([  96.9869, -100.1779], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(99.0824, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.0350, -99.8967], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.9659, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.0833, -99.6144], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.8489, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.1311, -99.3316], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.7313, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.1795, -99.0474], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.6135, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.2285, -98.7618], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.4952, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.2784, -98.4747], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.3766, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.3289, -98.1864], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.2577, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.3797, -97.8973], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.1385, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.4300, -97.6081], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(98.0190, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.4803, -97.3183], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(97.8993, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.5310, -97.0278], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(97.7794, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.5823, -96.7363], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(97.6593, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.6340, -96.4450], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(97.5395, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.6839, -96.1552], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(97.4195, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.7341, -95.8655], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(97.2998, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.7844, -95.6311], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(97.2077, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.8327, -95.2870], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(97.0599, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.8813, -94.9982], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(96.9397, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.9302, -94.7086], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(96.8194, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.9794, -94.4184], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(96.6989, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.0287, -94.1279], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(96.5783, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.0781, -93.8383], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(96.4582, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.1035, -93.5490], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(96.3263, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.8531, -93.2538], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(96.0535, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.2207, -92.9593], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(96.0900, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.8754, -92.8106], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.8430, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.8755, -92.6554], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.7654, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.8793, -92.4938], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.6866, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.8862, -92.3265], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.6063, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.8960, -92.1539], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.5250, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.9085, -91.9767], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.4426, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.9233, -91.7950], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.3591, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.9403, -91.6093], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.2748, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.9593, -91.4198], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.1895, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 97.9801, -91.2271], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.1036, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.0026, -91.0312], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(95.0169, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.0266, -90.8325], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.9295, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.0520, -90.6311], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.8416, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.0786, -90.4274], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.7530, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.1065, -90.2214], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.6639, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.1352, -90.0135], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.5744, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.1650, -89.8037], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.4844, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.1957, -89.5924], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.3940, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.2269, -89.3905], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.3087, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.2514, -89.1858], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.2186, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.2766, -88.9896], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.1331, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.3027, -88.7910], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(94.0469, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.3293, -88.5903], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.9598, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.3567, -88.3869], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.8718, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.3854, -88.1819], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.7836, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.4147, -87.9751], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.6949, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.4445, -87.7670], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.6057, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.4748, -87.5575], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.5161, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.5053, -87.3469], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.4261, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.5363, -87.1350], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.3357, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.5675, -86.9223], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.2449, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.5992, -86.7085], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.1538, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.6309, -86.4940], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(93.0624, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.6630, -86.2787], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.9709, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.6953, -86.0627], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.8790, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.7277, -85.8460], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.7869, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.7603, -85.6287], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.6945, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.7931, -85.4109], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.6020, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.8261, -85.1926], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.5093, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.8590, -84.9737], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.4164, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.8922, -84.7545], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.3233, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.9254, -84.5348], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.2301, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.9588, -84.3148], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.1368, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 98.9921, -84.0944], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(92.0433, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.0257, -83.8738], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.9498, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.0591, -83.6528], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.8559, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.0928, -83.4320], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.7624, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.1254, -83.2113], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.6684, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.1581, -82.9908], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.5744, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.1911, -82.7699], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.4805, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.2240, -82.5486], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.3863, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.2571, -82.3271], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.2921, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.2903, -82.1053], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.1978, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.3236, -81.8833], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.1034, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.3569, -81.6610], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(91.0089, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.3904, -81.4385], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.9144, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.4245, -81.3413], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.8829, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.4474, -81.0282], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.7378, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.4711, -80.8381], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.6546, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.4954, -80.6455], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.5704, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.5203, -80.4505], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.4854, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.5458, -80.2534], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.3996, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.5719, -80.0542], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.3130, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6247, -79.8536], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.2392, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.5975, -79.6973], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.1474, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.5996, -79.5354], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(90.0675, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6047, -79.3684], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.9865, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6123, -79.1969], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.9046, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6220, -79.0212], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.8216, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6339, -78.8418], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.7378, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6474, -78.6589], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.6532, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6627, -78.4730], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.5678, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6795, -78.2843], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.4819, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.6976, -78.0931], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.3954, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.7169, -77.8997], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.3083, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.7374, -77.7042], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.2208, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.7588, -77.5068], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.1328, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.7812, -77.3078], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(89.0445, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.8043, -77.1074], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(88.9558, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.8282, -76.9055], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(88.8669, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.8528, -76.7024], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(88.7776, grad_fn=<L1LossBackward0>)\nOUTPUT? tensor([ 99.8779, -76.4983], grad_fn=<ReshapeAliasBackward0>) torch.float32\nLABELS? tensor([0., 1.]) torch.float32\nLOSS? tensor(88.6881, grad_fn=<L1LossBackward0>)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3280910394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model(input_data['input_ids'],\n      attention_mask = input_data['attention_mask']\n     )[0] # test if we are really trained. not working?","metadata":{"execution":{"iopub.status.busy":"2022-12-30T21:58:56.265484Z","iopub.execute_input":"2022-12-30T21:58:56.265860Z","iopub.status.idle":"2022-12-30T21:58:56.334123Z","shell.execute_reply.started":"2022-12-30T21:58:56.265830Z","shell.execute_reply":"2022-12-30T21:58:56.333006Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor([[ 99.8982],\n        [-76.3190]], grad_fn=<IndexBackward0>)"},"metadata":{}}]}]}